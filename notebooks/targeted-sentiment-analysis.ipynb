{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-12-29T16:45:23.014520Z","iopub.status.busy":"2021-12-29T16:45:23.014179Z","iopub.status.idle":"2021-12-29T16:45:28.548184Z","shell.execute_reply":"2021-12-29T16:45:28.547448Z","shell.execute_reply.started":"2021-12-29T16:45:23.014432Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import nltk\n","import re\n","import string\n","import tensorflow as tf\n","from tensorflow.keras.layers import TextVectorization\n","from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, Conv2D, GlobalMaxPool2D, Layer\n","from tensorflow.keras import Model, Input\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Embedding\n","from tensorflow_addons.metrics import F1Score\n","import warnings\n","warnings.filterwarnings('ignore')\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:45:28.550117Z","iopub.status.busy":"2021-12-29T16:45:28.549874Z","iopub.status.idle":"2021-12-29T16:45:28.603716Z","shell.execute_reply":"2021-12-29T16:45:28.603050Z","shell.execute_reply.started":"2021-12-29T16:45:28.550083Z"},"trusted":true},"outputs":[],"source":["train = pd.read_csv('data/train.csv')\n","train.head(10)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:45:28.605435Z","iopub.status.busy":"2021-12-29T16:45:28.605045Z","iopub.status.idle":"2021-12-29T16:45:28.614525Z","shell.execute_reply":"2021-12-29T16:45:28.613627Z","shell.execute_reply.started":"2021-12-29T16:45:28.605399Z"},"trusted":true},"outputs":[],"source":["def preprocess(text):\n","\n","    # removing '\\n', '\\t'\n","    text = re.sub(\"\\n\", \"\", text)\n","    text = re.sub(\"\\t\", \"\", text)\n","    # lowercasing\n","    text = text.lower()\n","\n","    # specific\n","    text = re.sub(r\"won\\'t\", \"will not\", text)\n","    text = re.sub(r\"can\\'t\", \"can not\", text)\n","\n","    # general\n","    text = re.sub(r\"n\\'t\", \" not\", text)\n","    text = re.sub(r\"\\'re\", \" are\", text)\n","    text = re.sub(r\"\\'s\", \" is\", text)\n","    text = re.sub(r\"\\'d\", \" would\", text)\n","    text = re.sub(r\"\\'ll\", \" will\", text)\n","    text = re.sub(r\"\\'t\", \" not\", text)\n","    text = re.sub(r\"\\'ve\", \" have\", text)\n","    text = re.sub(r\"\\'m\", \" am\", text)\n","    \n","    # digits\n","    text = re.sub(r'\\d+?\\w', '', text)\n","\n","    # Remove some punctuations \n","    text = re.sub(r\"[!?,:'\\\"*)@#%(&$_.^-]\", ' ', text)\n","\n","    text = re.sub(' +', ' ', text)\n","\n","    return text"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:45:28.617615Z","iopub.status.busy":"2021-12-29T16:45:28.617092Z","iopub.status.idle":"2021-12-29T16:45:28.816655Z","shell.execute_reply":"2021-12-29T16:45:28.815918Z","shell.execute_reply.started":"2021-12-29T16:45:28.617571Z"},"trusted":true},"outputs":[],"source":["train['clean_text'] = train['text'].apply(preprocess)\n","train['clean_aspect'] = train['aspect'].apply(preprocess)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:45:28.818188Z","iopub.status.busy":"2021-12-29T16:45:28.817937Z","iopub.status.idle":"2021-12-29T16:45:28.825908Z","shell.execute_reply":"2021-12-29T16:45:28.824429Z","shell.execute_reply.started":"2021-12-29T16:45:28.818155Z"},"trusted":true},"outputs":[],"source":["def get_pw(k, m, i, n, mode=\"lf\"):\n","    C = 30.\n","\n","    i += 1\n","    k += 1\n","\n","    if i == k:\n","        pw = 1\n","    elif i < (k+m):\n","        pw = 1 - ((k + m - i)/C)\n","    elif (k + m) <= i and i <= n:\n","        pw = 1 - ((i - k)/C)\n","    else:\n","        pw = 0\n","\n","    return round(pw, ndigits=3) if pw > 0 else 0"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:45:28.828312Z","iopub.status.busy":"2021-12-29T16:45:28.827375Z","iopub.status.idle":"2021-12-29T16:45:28.838012Z","shell.execute_reply":"2021-12-29T16:45:28.837293Z","shell.execute_reply.started":"2021-12-29T16:45:28.828266Z"},"trusted":true},"outputs":[],"source":["def loop_pw(sentence, target):\n","    \n","    text = sentence.split(\" \")\n","    target_words = target.split(\" \")\n","\n","    first_target = [target_words[0]]\n","    m = len(first_target)\n","    n = len(text)\n","\n","    if first_target[0] not in text:\n","        for i in range(len(text)):\n","            if target_words[0] in text[i]:\n","                k = i\n","                break\n","\n","        pw = [get_pw(k, m, idx, n) for idx in range(len(text))]\n","            \n","\n","    else:\n","\n","        for i, word in enumerate(text):\n","            if target_words[0] in word:\n","\n","                if len(target_words[0]) < len(word):\n","                    continue\n","                else:\n","                    k = i\n","\n","        pw = [get_pw(k, m, idx, n) for idx in range(len(text))]\n","\n","    return pw"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:45:28.839685Z","iopub.status.busy":"2021-12-29T16:45:28.839229Z","iopub.status.idle":"2021-12-29T16:45:29.009606Z","shell.execute_reply":"2021-12-29T16:45:29.008868Z","shell.execute_reply.started":"2021-12-29T16:45:28.839647Z"},"trusted":true},"outputs":[],"source":["positionW = []\n","for i in range(0, len(train)):\n","    positionW.append(loop_pw(train['clean_text'][i], train['clean_aspect'][i]))\n","\n","positionW[0]"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:45:29.011291Z","iopub.status.busy":"2021-12-29T16:45:29.010924Z","iopub.status.idle":"2021-12-29T16:45:29.196039Z","shell.execute_reply":"2021-12-29T16:45:29.195301Z","shell.execute_reply.started":"2021-12-29T16:45:29.011255Z"},"trusted":true},"outputs":[],"source":["# Padding positionW to seq length\n","max_seq_length = 46\n","for i in range(len(positionW)):\n","\n","    if len(positionW[i]) < max_seq_length:\n","        pad_length = max_seq_length - len(positionW[i])\n","        positionW[i] = np.pad(positionW[i], (0, pad_length), 'constant')\n","\n","    else:\n","            positionW[i] = positionW[i][:max_seq_length]\n","\n","positionW[:3]\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:45:29.197658Z","iopub.status.busy":"2021-12-29T16:45:29.197388Z","iopub.status.idle":"2021-12-29T16:45:29.214729Z","shell.execute_reply":"2021-12-29T16:45:29.214045Z","shell.execute_reply.started":"2021-12-29T16:45:29.197623Z"},"trusted":true},"outputs":[],"source":["positionW = np.stack(arrays=positionW, axis=0)\n","positionW.shape"]},{"cell_type":"markdown","metadata":{},"source":["### Preprocess the text"]},{"cell_type":"markdown","metadata":{},"source":["**seq len = 46 (99 percentile) |**\n","**max aspect len = 8**"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:45:29.218468Z","iopub.status.busy":"2021-12-29T16:45:29.218267Z","iopub.status.idle":"2021-12-29T16:45:29.222317Z","shell.execute_reply":"2021-12-29T16:45:29.221692Z","shell.execute_reply.started":"2021-12-29T16:45:29.218444Z"},"trusted":true},"outputs":[],"source":["def tokenize_sent(text):\n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts(text)\n","\n","    return tokenizer, tokenizer.texts_to_sequences(text)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:45:29.224631Z","iopub.status.busy":"2021-12-29T16:45:29.224005Z","iopub.status.idle":"2021-12-29T16:45:29.367322Z","shell.execute_reply":"2021-12-29T16:45:29.366593Z","shell.execute_reply.started":"2021-12-29T16:45:29.224592Z"},"trusted":true},"outputs":[],"source":["tokenizer, _ = tokenize_sent(train.clean_text.to_list())"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:45:29.369020Z","iopub.status.busy":"2021-12-29T16:45:29.368772Z","iopub.status.idle":"2021-12-29T16:45:29.376228Z","shell.execute_reply":"2021-12-29T16:45:29.375297Z","shell.execute_reply.started":"2021-12-29T16:45:29.368988Z"},"trusted":true},"outputs":[],"source":["word2idx = tokenizer.word_index\n","\n","vocab_size = len(word2idx)\n","vocab_size"]},{"cell_type":"markdown","metadata":{},"source":["### Embedding using Glove"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:45:29.378316Z","iopub.status.busy":"2021-12-29T16:45:29.378049Z","iopub.status.idle":"2021-12-29T16:46:04.243799Z","shell.execute_reply":"2021-12-29T16:46:04.242997Z","shell.execute_reply.started":"2021-12-29T16:45:29.378280Z"},"trusted":true},"outputs":[],"source":["path_to_glove = '../input/glove6b/glove.6B.300d.txt'\n","\n","embeddings_index = {}\n","with open(path_to_glove) as f:\n","    for line in f:\n","        word, coefs = line.split(maxsplit=1)\n","        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n","        embeddings_index[word] = coefs\n","\n","print(\"Found %s word vectors.\" % len(embeddings_index))"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:46:04.245578Z","iopub.status.busy":"2021-12-29T16:46:04.245152Z","iopub.status.idle":"2021-12-29T16:46:06.864760Z","shell.execute_reply":"2021-12-29T16:46:06.864078Z","shell.execute_reply.started":"2021-12-29T16:46:04.245541Z"},"trusted":true},"outputs":[],"source":["num_tokens = vocab_size +1\n","embedding_dim = 300\n","hits = 0\n","misses = 0\n","\n","# Prepare embedding matrix\n","embedding_matrix = np.zeros((num_tokens, embedding_dim))\n","for word, i in word2idx.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        # Words not found in embedding index will be all-zeros.\n","        # This includes the representation for \"padding\" and \"OOV\"\n","        embedding_matrix[i] = embedding_vector\n","        hits += 1\n","    else:\n","        embedding_matrix[i] = tf.random.uniform(shape=(embedding_dim, ), minval=-0.25, maxval=0.25).numpy()\n","        misses += 1\n","print(\"Converted %d words (%d misses)\" % (hits, misses))"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:46:30.252336Z","iopub.status.busy":"2021-12-29T16:46:30.251585Z","iopub.status.idle":"2021-12-29T16:46:30.256764Z","shell.execute_reply":"2021-12-29T16:46:30.255638Z","shell.execute_reply.started":"2021-12-29T16:46:30.252295Z"},"trusted":true},"outputs":[],"source":["# hyper Parameters\n","hidden_dims = 50\n","filter_nums = 50"]},{"cell_type":"markdown","metadata":{},"source":["### Embedding Layer"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:46:30.867191Z","iopub.status.busy":"2021-12-29T16:46:30.866950Z","iopub.status.idle":"2021-12-29T16:46:30.893054Z","shell.execute_reply":"2021-12-29T16:46:30.891181Z","shell.execute_reply.started":"2021-12-29T16:46:30.867164Z"},"trusted":true},"outputs":[],"source":["embedding_layer = Embedding(\n","    num_tokens,\n","    embedding_dim,\n","    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n","    mask_zero=True,\n","    trainable=False,\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["embedding_layer -> (None, seq_len, 300)"]},{"cell_type":"markdown","metadata":{},"source":["### Bi-LSTM"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:46:31.812050Z","iopub.status.busy":"2021-12-29T16:46:31.811802Z","iopub.status.idle":"2021-12-29T16:46:31.816946Z","shell.execute_reply":"2021-12-29T16:46:31.816256Z","shell.execute_reply.started":"2021-12-29T16:46:31.812024Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.layers import Bidirectional, LSTM\n","\n","\n","def BiLSTM(hidden_nums, embed_out):\n","\n","\n","    lstm_layer = Bidirectional(LSTM(hidden_dims, return_sequences=True))\n","\n","    outputs = lstm_layer(embed_out)\n","\n","    return outputs"]},{"cell_type":"markdown","metadata":{},"source":["BiLstm  -> (1, seq_len, hidden_dims*2)"]},{"cell_type":"markdown","metadata":{},"source":["### Context Preserving transformation"]},{"cell_type":"markdown","metadata":{},"source":["1. Target Specific Transformation\n","2. Adaptive Scaling / Lossless Forwarding"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:46:32.858298Z","iopub.status.busy":"2021-12-29T16:46:32.857897Z","iopub.status.idle":"2021-12-29T16:46:32.877990Z","shell.execute_reply":"2021-12-29T16:46:32.876603Z","shell.execute_reply.started":"2021-12-29T16:46:32.858268Z"},"trusted":true},"outputs":[],"source":["class CPT(Layer):\n","    def __init__(self, hidden_nums):\n","        super(CPT, self).__init__()\n","        self.hidden_nums = hidden_nums\n","\n","        self.t_weights = {\n","            'trans_weights': tf.Variable(tf.initializers.RandomUniform(-0.01, 0.01)(shape=[4 * self.hidden_nums, 2*self.hidden_nums]),\n","                                        trainable=True,\n","                                        import_scope='TST_weights',\n","                                        name='trans_W')\n","        }\n","        self.t_bias = {\n","            'trans_bias': tf.Variable(tf.zeros_initializer()(shape=[2 * self.hidden_nums]),\n","                                        trainable=True,\n","                                        import_scope='TST_bias',\n","                                        name='trans_b')\n","        }\n","\n","    def tst(self, target_hidden_states, hidden_states):\n","        hidden_sp = tf.shape(hidden_states)\n","        batch_size = hidden_sp[0]\n","\n","        # (seq_len , batch_size, 2 * hidden_size)\n","        hs_ = tf.transpose(hidden_states, perm=[1, 0, 2])\n","        # (batch_size, 2*hidden_size, target_len)\n","        t_ = tf.transpose(target_hidden_states, perm=[0, 2, 1])\n","\n","        # tst\n","        sentence_index = 0\n","        sentence_array = tf.TensorArray(dtype=tf.float32, size=1, dynamic_size=True)\n","\n","        def body(sentence_index, sentence_array):\n","            # (batch_size, 2*hidden_size)\n","            hi = tf.transpose(tf.gather_nd(\n","                hs_, [[sentence_index]]), perm=[1, 2, 0])\n","\n","            # (batch_size, target_length)\n","            ai = tf.nn.softmax(tf.squeeze(\n","                tf.matmul(target_hidden_states, hi), axis=-1))\n","\n","            # (batch_size, 2 * hidden_size, 1)\n","            ti = tf.matmul(t_, tf.expand_dims(ai, axis=-1))\n","\n","            # squeeze_dim = 1\n","            hi = tf.squeeze(hi, axis=-1)\n","            ti = tf.squeeze(ti, axis=-1)\n","\n","            # concatenate (batch_size, 1, 4 * hidden_size)\n","            concated_hi = tf.concat([hi, ti], axis=-1)\n","            concated_hi = tf.reshape(\n","                concated_hi, [batch_size, 1, 4 * self.hidden_nums])\n","\n","\n","            hi_new = tf.math.tanh(\n","                tf.matmul(concated_hi, tf.tile(tf.expand_dims(self.t_weights['trans_weights'], axis=0),\n","                        [batch_size, 1, 1])) + self.t_bias['trans_bias']\n","            )\n","\n","            hi_new = tf.squeeze(hi_new, axis=1)\n","\n","            sentence_array = sentence_array.write(sentence_index, hi_new)\n","\n","            return (sentence_index + 1, sentence_array)\n","\n","        def cond(sentence_index, sentence_array):\n","            return sentence_index < hidden_sp[1]\n","\n","        _, sentence_array = tf.while_loop(\n","            cond=cond,\n","            body=body,\n","            loop_vars=[sentence_index, sentence_array])\n","\n","        # while sentence_index < hidden_sp[1]:\n","        #    sentence_index, sentence_array = body(sentence_index, sentence_array)\n","\n","        sentence_array = tf.transpose(sentence_array.stack(), perm=[1, 0, 2])\n","\n","        return sentence_array\n","\n","\n","    def lf_layer(self, target_hidden_states, hidden_states):\n","        hidden_states_ = self.tst(target_hidden_states, hidden_states)\n","\n","        return hidden_states_ + hidden_states\n","\n","    def call(self, target_hidden_states, hidden_states):\n","        \"\"\"\n","        Input : {\n","            target_embeddings: (?, ?, embed_dim),\n","            target_sequence_length : (?, ),\n","            hidden_states: (?, ?, 2 * hidden_nums)\n","        }\n","        \"\"\"\n","\n","        output = self.lf_layer(target_hidden_states, hidden_states)\n","\n","        return output\n","    \n","    def get_config(self):\n","        config = super(CPT, self).get_config()\n","        config.update({\n","            'hidden_nums': self.hidden_nums,\n","        })\n","        return config\n","    \n","    @classmethod\n","    def from_config(cls, config):\n","        return cls(**config)"]},{"cell_type":"markdown","metadata":{},"source":["### Convulation Layer"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:46:33.492259Z","iopub.status.busy":"2021-12-29T16:46:33.492018Z","iopub.status.idle":"2021-12-29T16:46:33.500010Z","shell.execute_reply":"2021-12-29T16:46:33.499077Z","shell.execute_reply.started":"2021-12-29T16:46:33.492232Z"},"trusted":true},"outputs":[],"source":["class CnnLayer(tf.keras.layers.Layer):\n","    def __init__(self, filter_nums, kernel_size):\n","        super(CnnLayer, self).__init__()\n","        self.kernel_size = kernel_size\n","        self.filter_nums = filter_nums\n","\n","        self.cnn_layer = Conv2D(self.filter_nums, self.kernel_size, activation='relu')\n","        self.pool_layer = GlobalMaxPool2D()\n","\n","    def call(self, hidden_states):\n","        hs = tf.expand_dims(hidden_states, axis=-1)\n","        features = self.cnn_layer(hs)\n","        outputs = self.pool_layer(features)\n","\n","        return outputs, features\n","    \n","    def get_config(self):\n","        config = super(CnnLayer, self).get_config()\n","        config.update({\n","            'filter_nums': self.filter_nums,\n","            'kernel_size': self.kernel_size\n","        })\n","        return config\n","\n","    @classmethod\n","    def from_config(cls, config):\n","        return cls(**config)"]},{"cell_type":"markdown","metadata":{},"source":["### Positional relavance"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:46:34.150927Z","iopub.status.busy":"2021-12-29T16:46:34.150419Z","iopub.status.idle":"2021-12-29T16:46:34.155552Z","shell.execute_reply":"2021-12-29T16:46:34.154526Z","shell.execute_reply.started":"2021-12-29T16:46:34.150891Z"},"trusted":true},"outputs":[],"source":["def position_embedding(hs, pw):\n","        \"\"\"\n","        @hs: (batch_size, sentence_length, 2 * hidden_nums)\n","        @pw: (batch_size, sentence_length)\n","        \"\"\"\n","        weighted_hs = hs * tf.expand_dims(pw, axis=-1)\n","\n","        return weighted_hs"]},{"cell_type":"markdown","metadata":{},"source":["### Model"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:46:37.491331Z","iopub.status.busy":"2021-12-29T16:46:37.490953Z","iopub.status.idle":"2021-12-29T16:46:41.346993Z","shell.execute_reply":"2021-12-29T16:46:41.346159Z","shell.execute_reply.started":"2021-12-29T16:46:37.491297Z"},"trusted":true},"outputs":[],"source":["\n","# sentence input\n","\n","sentence_input = Input(shape=(None, ), name='sentence_input')\n","sentence_embed = embedding_layer(sentence_input)\n","sentence_hidden_states = BiLSTM(hidden_dims, sentence_embed)\n","sentence_hidden_states = tf.keras.layers.Dropout(0.3)(sentence_hidden_states)\n","\n","# target input\n","target_input = Input(shape=(None, ), name='target_input')\n","target_embed = embedding_layer(target_input)\n","target_hidden_states = BiLSTM(hidden_dims, target_embed)\n","target_hidden_states = tf.keras.layers.Dropout(0.3)(target_hidden_states)\n","\n","# CPT 1\n","cpt_out_1 = CPT(hidden_dims)(target_hidden_states, sentence_hidden_states)\n","\n","# position weighting\n","pw = Input(shape=(None,), name='pw')\n","modified_hidden_states_1 = position_embedding(cpt_out_1, pw)\n","\n","# CPT 2\n","cpt_out_2 = CPT(hidden_dims)(target_hidden_states, modified_hidden_states_1)\n","\n","# position weighting\n","modified_hidden_states_2 = position_embedding(cpt_out_2, pw)\n","\n","# CNN layer\n","cnn_output, cnn_features = CnnLayer(filter_nums, 3)(modified_hidden_states_2)\n","\n","# Dropout\n","drp_out = tf.keras.layers.Dropout(0.3)(cnn_output)\n","\n","# output layer\n","output_layer = Dense(3, activation='softmax')(drp_out)\n","\n","model = Model(inputs=[sentence_input, target_input, pw], outputs=output_layer)"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:46:41.349104Z","iopub.status.busy":"2021-12-29T16:46:41.348895Z","iopub.status.idle":"2021-12-29T16:46:41.362366Z","shell.execute_reply":"2021-12-29T16:46:41.361587Z","shell.execute_reply.started":"2021-12-29T16:46:41.349075Z"},"trusted":true},"outputs":[],"source":["model.compile(loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2),\n","             optimizer = tf.keras.optimizers.Adam(),\n","             metrics=['accuracy', F1Score(3)])"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:47:49.111228Z","iopub.status.busy":"2021-12-29T16:47:49.110967Z","iopub.status.idle":"2021-12-29T16:47:49.128301Z","shell.execute_reply":"2021-12-29T16:47:49.127473Z","shell.execute_reply.started":"2021-12-29T16:47:49.111193Z"},"trusted":true},"outputs":[],"source":["model.summary()"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:47:57.524834Z","iopub.status.busy":"2021-12-29T16:47:57.524572Z","iopub.status.idle":"2021-12-29T16:47:57.535112Z","shell.execute_reply":"2021-12-29T16:47:57.534417Z","shell.execute_reply.started":"2021-12-29T16:47:57.524805Z"},"trusted":true},"outputs":[],"source":["# Making validation split\n","cutoff = 0.95\n","split = int(len(train) * cutoff)\n","# spliting texts\n","train_text = train.clean_text[: split].to_list()\n","val_text = train.clean_text[split : ].to_list()\n","\n","# spliting aspects\n","train_aspect = train.clean_aspect[: split].to_list()\n","val_aspect = train.clean_aspect[split : ].to_list()\n","\n","# split positionW\n","train_pw = positionW[: split]\n","val_pw = positionW[split : ]\n","\n","# split labels\n","train_labels = train.label[: split]\n","val_labels = train.label[split :]\n","\n","len(train_text), len(train_aspect), len(train_pw), len(train_labels), len(val_text), len(val_aspect), len(val_pw), len(val_labels)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:47:59.945529Z","iopub.status.busy":"2021-12-29T16:47:59.944945Z","iopub.status.idle":"2021-12-29T16:48:00.034332Z","shell.execute_reply":"2021-12-29T16:48:00.033588Z","shell.execute_reply.started":"2021-12-29T16:47:59.945494Z"},"trusted":true},"outputs":[],"source":["train_p_text = tokenizer.texts_to_sequences(train_text)\n","val_p_text = tokenizer.texts_to_sequences(val_text)\n","\n","train_p_target = tokenizer.texts_to_sequences(train_aspect)\n","val_p_target = tokenizer.texts_to_sequences(val_aspect)"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:48:01.200948Z","iopub.status.busy":"2021-12-29T16:48:01.200294Z","iopub.status.idle":"2021-12-29T16:48:01.277819Z","shell.execute_reply":"2021-12-29T16:48:01.277124Z","shell.execute_reply.started":"2021-12-29T16:48:01.200912Z"},"trusted":true},"outputs":[],"source":["# padding\n","seq_len = 46\n","aspect_len = 8\n","train_p_text = pad_sequences(train_p_text, maxlen=seq_len, padding=\"post\", truncating='post', value=0)\n","val_p_text = pad_sequences(val_p_text, maxlen=seq_len, padding=\"post\", truncating='post', value=0)\n","\n","train_p_target = pad_sequences(train_p_target, maxlen=aspect_len, padding=\"post\", truncating='post', value=0)\n","val_p_target = pad_sequences(val_p_target, maxlen=aspect_len, padding=\"post\", truncating='post', value=0)"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:48:02.009047Z","iopub.status.busy":"2021-12-29T16:48:02.008550Z","iopub.status.idle":"2021-12-29T16:48:02.014069Z","shell.execute_reply":"2021-12-29T16:48:02.013420Z","shell.execute_reply.started":"2021-12-29T16:48:02.009012Z"},"trusted":true},"outputs":[],"source":["train_p_text.shape, train_p_target.shape"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:48:02.233560Z","iopub.status.busy":"2021-12-29T16:48:02.232822Z","iopub.status.idle":"2021-12-29T16:48:02.240767Z","shell.execute_reply":"2021-12-29T16:48:02.240010Z","shell.execute_reply.started":"2021-12-29T16:48:02.233512Z"},"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import OneHotEncoder\n","ohe = OneHotEncoder(sparse=False)\n","train_ohe_labels = ohe.fit_transform(train_labels.to_numpy().reshape(-1, 1))\n","val_ohe_labels = ohe.transform(val_labels.to_numpy().reshape(-1, 1))"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:48:02.426568Z","iopub.status.busy":"2021-12-29T16:48:02.426060Z","iopub.status.idle":"2021-12-29T16:48:02.452004Z","shell.execute_reply":"2021-12-29T16:48:02.451375Z","shell.execute_reply.started":"2021-12-29T16:48:02.426535Z"},"trusted":true},"outputs":[],"source":["# train dataset\n","train_inputs = tf.data.Dataset.from_tensor_slices((train_p_text,\n","                                                train_p_target,\n","                                                train_pw))\n","\n","train_labels = tf.data.Dataset.from_tensor_slices(train_ohe_labels)\n","\n","\n","train_data = tf.data.Dataset.zip((train_inputs, train_labels))\n","\n","train_data = train_data.batch(64).prefetch(tf.data.AUTOTUNE)\n","\n","\n","# Val dataset\n","val_inputs = tf.data.Dataset.from_tensor_slices((val_p_text,\n","                                                val_p_target,\n","                                                val_pw))\n","\n","val_labels = tf.data.Dataset.from_tensor_slices(val_ohe_labels)\n","\n","\n","val_data = tf.data.Dataset.zip((val_inputs, val_labels))\n","\n","val_data = val_data.batch(64).prefetch(tf.data.AUTOTUNE)"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:48:03.588714Z","iopub.status.busy":"2021-12-29T16:48:03.588177Z","iopub.status.idle":"2021-12-29T16:48:03.595545Z","shell.execute_reply":"2021-12-29T16:48:03.593722Z","shell.execute_reply.started":"2021-12-29T16:48:03.588677Z"},"trusted":true},"outputs":[],"source":["train_data, val_data"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:48:18.856523Z","iopub.status.busy":"2021-12-29T16:48:18.856197Z","iopub.status.idle":"2021-12-29T16:50:29.909037Z","shell.execute_reply":"2021-12-29T16:50:29.908283Z","shell.execute_reply.started":"2021-12-29T16:48:18.856487Z"},"trusted":true},"outputs":[],"source":["early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.005, patience=3, mode='max', baseline=0.69)\n","\n","\n","history = model.fit(train_data, epochs=25, validation_data=val_data, callbacks=[early_stopping])"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T16:53:33.545095Z","iopub.status.busy":"2021-12-29T16:53:33.544837Z","iopub.status.idle":"2021-12-29T16:54:22.550126Z","shell.execute_reply":"2021-12-29T16:54:22.549376Z","shell.execute_reply.started":"2021-12-29T16:53:33.545066Z"},"trusted":true},"outputs":[],"source":["model.save('Tnet-LF-300dGLOVE')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","def plot_loss_curves(history):\n","    \"\"\"\n","    Returns separate loss curves for training and validation metrics.\n","    Args:\n","    history: TensorFlow model History object (see: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History)\n","    \"\"\" \n","    loss = history.history['loss']\n","    val_loss = history.history['val_loss']\n","\n","    accuracy = history.history['accuracy']\n","    val_accuracy = history.history['val_accuracy']\n","    \n","    f1_score = history.history['f1_score']\n","    val_f1_score = history.history['f1_score']\n","\n","    epochs = range(len(history.history['loss']))\n","\n","    # Plot loss\n","    plt.plot(epochs, loss, label='training_loss')\n","    plt.plot(epochs, val_loss, label='val_loss')\n","    plt.title('Loss')\n","    plt.xlabel('Epochs')\n","    plt.legend()\n","\n","    # Plot accuracy\n","    plt.figure()\n","    plt.plot(epochs, accuracy, label='training_accuracy')\n","    plt.plot(epochs, val_accuracy, label='val_accuracy')\n","    plt.title('Accuracy')\n","    plt.xlabel('Epochs')\n","    plt.legend();\n","    \n","    # Plot F1-score\n","    plt.figure()\n","    plt.plot(epochs, accuracy, label='train_f1_score')\n","    plt.plot(epochs, val_accuracy, label='val_f1_score')\n","    plt.title('Accuracy')\n","    plt.xlabel('Epochs')\n","    plt.legend();\n","    \n","plot_loss_curves(history)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test = pd.read_csv('../input/tsadata/test.csv')\n","test.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test['clean_text'] = test['text'].apply(preprocess)\n","test['clean_aspect'] = test['aspect'].apply(preprocess)\n","\n","test.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pw_t = []\n","seq_len = 46\n","aspect_len = 8\n","\n","for i in range(len(test)):\n","    pw_t.append(loop_pw(test['clean_text'][i], test['clean_aspect'][i]))\n","\n","for i in range(len(test)):\n","    if len(pw_t[i]) < seq_len:\n","        pad_length = seq_len - len(pw_t[i])\n","        pw_t[i] = np.pad(pw_t[i], (0, pad_length), 'constant')\n","\n","    else:\n","            pw_t[i] = pw_t[i][:seq_len]\n","        \n","\n","pw_t = np.stack(arrays=pw_t, axis=0)\n","pw_t.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tqdm import tqdm\n","test_label = []\n","for i in tqdm(range(len(test))):\n","    sentence, target = test['clean_text'][i], test['clean_aspect'][i]\n","\n","    sen = tokenizer.texts_to_sequences([sentence])\n","    tar = tokenizer.texts_to_sequences([target])\n","\n","    sen = pad_sequences(sen, maxlen=seq_len, padding=\"post\", truncating='post', value=0)\n","    tar = pad_sequences(tar, maxlen=aspect_len, padding=\"post\", truncating='post', value=0)\n","\n","    probs = model.predict([sen, tar, pw_t[i].reshape(1, -1)])\n","    test_label.append(np.argmax(probs, axis=1)[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["result = {\n","    'text' : test['text'].to_list(),\n","    'aspect' : test['aspect'].to_list(),\n","    'label' : test_label\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["result = pd.DataFrame(result)\n","result.to_csv('test.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":4}
